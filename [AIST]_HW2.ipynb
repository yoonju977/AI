{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoonju977/AI/blob/main/%5BAIST%5D_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9bH0stL3lu7"
      },
      "source": [
        "# *Homework #2*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stP6vzDZ3lu8"
      },
      "source": [
        "**이름과 학번을 기재해 주세요**\n",
        "- Name : ooo\n",
        "- Student Id : 202512345\n",
        "- Submission date : 2024 / 00 / 00"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Oqx3IDVa3lu9"
      },
      "outputs": [],
      "source": [
        "# 이름과 학번을 기재해 주세요\n",
        "NAME = \"송윤주\"\n",
        "ID = \"201701881\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKaHHdcS3lu9",
        "outputId": "7fca8dc9-332b-4220-b363-0cdf90815313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:HELLO WORLD!!\n"
          ]
        }
      ],
      "source": [
        "# NOTE - 해당 셀 수정 금지\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "file_handler = logging.FileHandler(f'hw2_{ID}_{NAME}.log')\n",
        "file_handler.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "file_handler.setFormatter(formatter)\n",
        "logger.addHandler(file_handler)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "logger.info('HELLO WORLD!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fODsNdQP3lu-",
        "outputId": "d24890f1-9675-4038-cfd1-37eaad99a788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Set seed\n"
          ]
        }
      ],
      "source": [
        "# NOTE - 해당 셀 수정 금지\n",
        "def set_seed(seed):\n",
        "    logger.info(\"Set seed\")\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "SEED = 42\n",
        "set_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_crMO5v_3lu-",
        "outputId": "bdc52802-fdd4-4bc3-a6de-b1e6d50df6cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Q1\n"
          ]
        }
      ],
      "source": [
        "# NOTE - 해당 셀 수정 금지\n",
        "logger.info(\"Q1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s_qToD53lu-"
      },
      "source": [
        "## Q1.1. 다중 조건부 확률 계산을 위한 함수 구현 (20점)\n",
        "다음의 확률 이론들에 대한 설명을 참조하여, 어떤 사건 A와 조건들 B,C,...에 대해,  \n",
        "`The Law of Total Probability`, `Chain rule`, `Bayes' Theorem`을 이용하여 다중 조건부 확률을 계산하는 함수를 구현합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4GNSw963lu-"
      },
      "source": [
        "#### 1. Chain Rule (체인 룰)\n",
        "\n",
        "체인 룰은 여러 사건이 연속적으로 일어나는 확률을 각 사건의 조건부 확률로 분해해 계산할 때 사용됩니다. 체인 룰의 기본 형태는 아래와 같습니다.\n",
        "\n",
        "- 예를 들어, 두 사건 $A$ 와 $B$ 에 대해 체인 룰은 다음과 같이 나타낼 수 있습니다.\n",
        "  $$\n",
        "  P(A, B) = P(B \\mid A) \\cdot P(A)\n",
        "  $$\n",
        "\n",
        "- 여러 사건이 주어진 경우 체인 룰은 다음과 같이 확장됩니다.\n",
        "  $$\n",
        "  P(A, B, C, D) = P(D \\mid A, B, C) \\cdot P(C \\mid A, B) \\cdot P(B \\mid A) \\cdot P(A)\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. The Law of Total Probability (전체확률의 법칙)\n",
        "\n",
        "전확률의 법칙은 사건 $B$ 의 확률을 조건부 확률로 분해하여 구할 수 있는 법칙입니다. 이 법칙은 다음과 같은 수식으로 나타낼 수 있습니다:\n",
        "\n",
        "- 사건 $A$ 와 사건 $A$ 가 일어나지 않을 확률 $\\text{not A}$ 가 주어졌을 때,\n",
        "  $$\n",
        "  P(B) = P(B \\mid A) \\cdot P(A) + P(B \\mid \\text{not A}) \\cdot P(\\text{not A})\n",
        "  $$\n",
        "\n",
        "[wikipedia - 전체확률의 법칙](https://ko.wikipedia.org/wiki/%EC%A0%84%EC%B2%B4_%ED%99%95%EB%A5%A0%EC%9D%98_%EB%B2%95%EC%B9%99)\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Bayes' Theorem (베이즈 정리)\n",
        "베이즈 정리는 조건부 확률을 뒤집어 계산할 때 사용됩니다. 사건 $B$ 가 주어졌을 때 사건 $A$ 가 발생할 확률을 구할 수 있으며, 전체확률의 법칙을 바탕으로 계산됩니다.\n",
        "\n",
        "$$\n",
        "P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "{\\frac {P(B|A_{1})P(A_{1})}{P(B)}} =\n",
        "{\\frac {P(B|A_{1})P(A_{1})}{P(B|A_{1})P(A_{1})+P(B|A_{2})P(A_{2})}}\n",
        "$$\n",
        "\n",
        "[wikipedia - 베이즈 정리](https://ko.wikipedia.org/wiki/%EB%B2%A0%EC%9D%B4%EC%A6%88_%EC%A0%95%EB%A6%AC)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 4. conditional probability with multiple conditions (다중 조건부 확률)\n",
        "\n",
        "다중 조건부 확률은 사건 $A$ 가 여러 조건들 $B, C, D, ...$ 아래에서 발생할 확률을 의미합니다.   \n",
        "예를 들어, $A$ 가 $B$ 와 $C$ 가 동시에 일어났을 때 발생할 확률 $P(A \\mid B, C)$ 을 계산하는 것이 다중 조건부 확률 계산입니다.   \n",
        "이때 각 조건들이 순차적으로 주어졌다고 생각하고 체인 룰을 통해 확률을 분해하여 계산할 수 있습니다.  \n",
        "\n",
        "1. 먼저 $P(A \\mid B, C)$ 와 같은 확률을 구하려고 할 때, 체인 룰을 이용하여 이를 분해할 수 있습니다.\n",
        "   $$\n",
        "   P(A \\mid B, C) = \\frac{P(A, B, C)}{P(B, C)}\n",
        "   $$\n",
        "\n",
        "2. 체인 룰을 이용해 $P(A, B, C)$ 를 계산합니다.\n",
        "\n",
        "   $$\n",
        "   P(A, B, C) = P(A \\mid B, C) \\cdot P(B \\mid C) \\cdot P(C)\n",
        "   $$\n",
        "\n",
        "3. 이렇게 계산한 확률들을 통해 전체 확률을 구하고, 다중 조건부 확률을 계산할 수 있습니다.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OcdGlsE93lu-"
      },
      "outputs": [],
      "source": [
        "def chain_rule(probabilities):\n",
        "    \"\"\"\n",
        "    TODO - 체인 룰을 사용하여 다중 조건부 확률을 계산\n",
        "    - probabilities: 조건부 확률들의 리스트\n",
        "    예: [P(A|B), P(B|C), P(C|D), ...]\n",
        "    \"\"\"\n",
        "    joint_probability = 1.0\n",
        "    for prob in probabilities:\n",
        "        joint_probability *= prob\n",
        "    return joint_probability\n",
        "\n",
        "def law_of_total_probability(prior_A, likelihood_conditions_given_A, likelihood_conditions_given_not_A):\n",
        "    \"\"\"\n",
        "    TODO - 전체 확률의 법칙을 사용하여 P(B, C, ...)를 계산\n",
        "    - prior_A: 사건 A의 사전 확률\n",
        "    - likelihood_conditions_given_A: 사건 A 하에서의 조건부 확률 리스트 [P(B|A), P(C|B, A), ...]\n",
        "    - likelihood_conditions_given_not_A: 사건 A' 하에서의 조건부 확률 리스트 [P(B|A'), P(C|B, A'), ...]\n",
        "    \"\"\"\n",
        "    # P(B, C, ... | A) 계산\n",
        "    joint_given_A = chain_rule(likelihood_conditions_given_A)\n",
        "    # P(B, C, ... | A') 계산\n",
        "    joint_given_not_A = chain_rule(likelihood_conditions_given_not_A)\n",
        "    # 전체 확률의 법칙 적용\n",
        "    joint_likelihood = prior_A * joint_given_A + (1 - prior_A) * joint_given_not_A\n",
        "    return joint_likelihood\n",
        "\n",
        "def bayes_theorem(prior_A, likelihood_conditions_given_A, joint_likelihood_conditions):\n",
        "    \"\"\"\n",
        "    TODO - 베이즈 정리를 통해 다중 조건 하에서 조건부 확률을 계산\n",
        "    - prior_A: 사건 A의 사전 확률\n",
        "    - likelihood_conditions_given_A: 사건 A 하에서 조건부 확률 리스트 [P(B|A), P(C|B), ...]\n",
        "    - joint_likelihood_conditions: 다중 조건의 결합 확률 P(B, C, ...)\n",
        "    \"\"\"\n",
        "    # 체인 룰을 이용해 P(B, C, ... | A)를 계산\n",
        "    joint_given_A = chain_rule(likelihood_conditions_given_A)\n",
        "\n",
        "    # 베이즈 정리를 적용해 조건부 확률 계산\n",
        "    posterior = (joint_given_A * prior_A) / joint_likelihood_conditions\n",
        "    return posterior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wUl-Xhu3lu_"
      },
      "source": [
        "## Q 1.2. (15점)\n",
        "어떤 병에 걸릴 확률은 5%입니다. 특정 병이 있는 경우 증상 A, 증상 B, 증상 C가 각각 나타날 확률은 다음과 같습니다:\n",
        "\n",
        "- 질병이 있을 때 : `P(A∣질병)=0.8`,  `P(B∣A,질병)=0.7`, `P(C∣B,A,질병)=0.6`\n",
        "- 질병이 없을 때 : `P(A∣질병없음)=0.1`, `P(B∣A,질병없음)=0.2`, `P(C∣B,A,질병없음)=0.3`\n",
        "\n",
        "다음의 조건을 만족하는 `질병에 걸렸을 확률 : P(질병∣A,B,C)` 를 직접 계산하고 풀이과정을 서술하세요.\n",
        "* 어떤 확률이론을 '왜'썼는지 구체적으로 서술하셔야 합니다.\n",
        "* 최종 결과는 소수점 아래 다섯째 자리에서 반올림하여, 넷째 자리까지만 써주시면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG4rmFtL3lu_"
      },
      "source": [
        "## A 1.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYo959un3lu_"
      },
      "source": [
        "## Q 1.3. (5점)\n",
        "Q 1.2.의 문제를 **위에서 구현한 함수를 이용하여** 계산하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YWB9WFn3lu_",
        "outputId": "b289a371-6830-4fe1-bbbd-381d2872f26e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(질병|A, B, C) : 0.7467\n"
          ]
        }
      ],
      "source": [
        "# TODO - Q 1.1에서 작성한 함수를 이용하여 구현\n",
        "prior_disease = 0.05\n",
        "likelihood_conditions_given_disease = [0.8, 0.7, 0.6]\n",
        "likelihood_conditions_given_no_disease = [0.1, 0.2, 0.3]\n",
        "# 전체 확률의 법칙을 통해 P(A, B, C) 계산\n",
        "joint_likelihood_conditions = law_of_total_probability(prior_disease, likelihood_conditions_given_disease, likelihood_conditions_given_no_disease)\n",
        "# 베이즈 정리를 통해 P(질병|A, B, C) 계산\n",
        "posterior = bayes_theorem(prior_disease, likelihood_conditions_given_disease, joint_likelihood_conditions)\n",
        "\n",
        "print(f\"P(질병|A, B, C) : {posterior:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jO_SOtj3lu_"
      },
      "outputs": [],
      "source": [
        "# NOTE - 해당 셀 수정 금지\n",
        "logger.info(\"Q2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6VZaRNp3lu_"
      },
      "source": [
        "## Q 2.1. Simple GPT 모델 구현 (40점)\n",
        "다음의 class 들의 빈칸을 채워 최종적으로 Simplified Generative Pre-trained Transformer (GPT) 모델을 구현하세요.  \n",
        "\n",
        "코드의 빈칸은 Q1부터 Q15까지 총 15개입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1bZMqfaw3lu_"
      },
      "outputs": [],
      "source": [
        "# NOTE - Positional Encoding, 위치 인코딩을 통해 시퀀스의 위치 정보를 제공함\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        # NOTE - d_model은 트랜스포머 모델의 임베딩 차원 수\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # TODO - Q1 : 위치 인코딩을 저장할 행렬 생성\n",
        "        positional_embedding_matrix = torch.zeros(max_len, d_model)\n",
        "        # TODO - Q2 : 위치 벡터 생성\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        # 각 차원에 대한 주기적인 함수 적용을 위한 스케일링 값 계산\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        # TODO - Q3 : 짝수 인덱스에는 사인 함수 적용\n",
        "        positional_embedding_matrix[:, 0::2] = torch.sin(position * div_term)\n",
        "        # TODO - Q3 : 홀수 인덱스에는 코사인 함수 적용\n",
        "        positional_embedding_matrix[:, 1::2] = torch.cos(position * div_term)\n",
        "        # 배치 차원을 추가하여 모양 맞춤\n",
        "        positional_embedding_matrix = positional_embedding_matrix.unsqueeze(0)\n",
        "        # 모델이 학습하지 않도록 고정된 상태로 위치 인코딩 저장\n",
        "        self.register_buffer('positional_embedding_matrix', positional_embedding_matrix)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO - Q4 :입력 텐서에 위치 인코딩 더하기\n",
        "        x = x + self.positional_embedding_matrix[:, :x.size(1)]\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "F3yJEhEj3lvA"
      },
      "outputs": [],
      "source": [
        "# NOTE - Multi-Head Attention, 멀티 헤드 어텐션을 통해 여러 어텐션을 병렬로 수행하여 더 풍부한 표현 학습\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        # NOTE - d_model은 트랜스포머 모델의 임베딩 차원 수\n",
        "        # NOTE - num_heads는 어텐션 헤드의 수\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        # TODO - Q4 : self.d_k는 멀티헤드 어텐션에서 각 어텐션 헤드의 차원 수\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # TODO - Q5 : 가중치 행렬 초기화\n",
        "        self.W_q = nn.Linear(d_model, d_model)  # Query 변환\n",
        "        self.W_k = nn.Linear(d_model, d_model)  # Key 변환\n",
        "        self.W_v = nn.Linear(d_model, d_model)  # Value 변환\n",
        "        self.W_o = nn.Linear(d_model, d_model)  # 출력 변환\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        # 1. 선형 변환과 헤드 분할\n",
        "        # TODO - Q6 : (batch_size, seq_length, d_model) -> (batch_size, num_heads, seq_length, d_k)\n",
        "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # TODO - Q7 :2. Scaled Dot-product 어텐션 스코어 계산\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float))\n",
        "\n",
        "        # 3. 마스킹 적용 (필요한 경우)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # 4. 소프트맥스 적용하여 어텐션 가중치 계산\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # TODO - Q8 : 5. Value와 가중치를 곱하여 최종 출력 계산\n",
        "        out = torch.matmul(attention, V)\n",
        "        # 6. 헤드 연결과 원래 형태로 변환\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.W_o(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iCqqXUQ03lvA"
      },
      "outputs": [],
      "source": [
        "# NOTE - Transformer Decoder Block, 트랜스포머 디코더 블록, 멀티 헤드 어텐션과 피드 포워드 네트워크로 구성됨\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        # 멀티 헤드 어텐션 레이어\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        # TODO - Q9 : 레이어 정규화\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        # TODO - Q10 : 피드 포워드 네트워크\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * 4),  # 확장\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model * 4, d_model)   # 압축\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # 1. 멀티 헤드 어텐션 + 잔차 연결과 정규화\n",
        "        attention = self.attention(x, x, x, mask)\n",
        "        x = self.norm1(x + attention)\n",
        "        # TODO - Q11 : 2. 피드 포워드 + 잔차 연결과 정규화\n",
        "        ff = self.feed_forward(x)\n",
        "        x = self.norm2(x + ff)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Q-10Dt1z3lvA"
      },
      "outputs": [],
      "source": [
        "# GPT 단순화 버전\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len):\n",
        "        super(GPT, self).__init__()\n",
        "        # TODO - Q12 : 입력 토큰에 대한 임베딩\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        # 위치 인코딩 추가\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        # 단순화된 트랜스포머 디코더 블록 (num_layers 개 사용)\n",
        "        self.layers = nn.ModuleList([TransformerBlock(d_model, num_heads) for _ in range(num_layers)])\n",
        "        # TODO - Q13 : 출력층 정의 (다음 단어 예측)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 임베딩과 위치 인코딩을 입력에 적용\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        # 트랜스포머 블록 통과시킴\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        # 마지막으로 출력층을 통해 다음 단어에 대한 로짓 계산\n",
        "        logits = self.fc_out(x)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "clvfFYNk3lvA"
      },
      "outputs": [],
      "source": [
        "# HACK - Hyperparameters, 모델의 임의의 하이퍼파라미터 설정\n",
        "vocab_size = 1000  # 임의의 vocab size\n",
        "d_model = 128\n",
        "num_heads = 4  # num_heads 값을 설정\n",
        "d_ff = 256  # 피드 포워드 네트워크의 차원 수 설정\n",
        "num_layers = 1  # 레이어 수\n",
        "max_len = 50\n",
        "epochs = 70\n",
        "learning_rate = 0.001  # 학습률 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "B__0JQ1S3lvA"
      },
      "outputs": [],
      "source": [
        "# HACK - 데이터 예시 (간단한 토큰 시퀀스)\n",
        "data = [\n",
        "    random.sample(range(vocab_size), 10)\n",
        "    for _ in range(1000)\n",
        "    ]\n",
        "\n",
        "# 데이터셋 준비\n",
        "# 입력 토큰 시퀀스를 텐서로 변환\n",
        "tokens = torch.tensor(data, dtype=torch.long)\n",
        "# 라벨도 동일하게 입력과 같은 데이터로 사용 (다음 단어 예측을 위해)\n",
        "labels = torch.tensor(data, dtype=torch.long)\n",
        "\n",
        "# 모델 초기화\n",
        "model = GPT(vocab_size, d_model, num_heads, d_ff, num_layers, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0YPAtsO3lvA",
        "outputId": "ebf84c49-0dc1-499f-cad1-21d8775cb9c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/70], Loss: 7.0693\n",
            "Epoch [11/70], Loss: 5.5995\n",
            "Epoch [21/70], Loss: 4.0732\n",
            "Epoch [31/70], Loss: 2.6954\n",
            "Epoch [41/70], Loss: 1.5389\n",
            "Epoch [51/70], Loss: 0.7374\n",
            "Epoch [61/70], Loss: 0.3282\n",
            "Training finished!\n"
          ]
        }
      ],
      "source": [
        "# 손실 함수 정의 (크로스 엔트로피 손실)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# 옵티마이저 정의 (Adam 옵티마이저 사용)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 모델 훈련\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()  # 기울기 초기화\n",
        "    output = model(tokens)  # 모델에 입력 데이터 전달\n",
        "    # 손실 계산 (출력과 라벨을 비교)\n",
        "    loss = criterion(output.view(-1, vocab_size), labels.view(-1))\n",
        "    # TODO - Q14 : 역전파를 통해 기울기 계산\n",
        "    loss.backward()\n",
        "    optimizer.step()  # 옵티마이저로 파라미터 업데이트\n",
        "    if epoch%10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"Training finished!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "N7mh5I2Q3lvA"
      },
      "outputs": [],
      "source": [
        "# 텍스트 생성 예시\n",
        "vocab = {str(i): i for i in range(vocab_size)}\n",
        "vocab_reverse = {i: str(i) for i in range(vocab_size)}\n",
        "\n",
        "# NOTE - 간단한 텍스트 생성 함수 호출\n",
        "def generate_text(model, start_text, vocab, vocab_reverse, max_length=100, temperature=1.0):\n",
        "    \"\"\"텍스트 생성 함수\"\"\"\n",
        "    model.eval()\n",
        "    current_text = start_text\n",
        "    # 시작 텍스트를 인코딩\n",
        "    encoded = torch.tensor([[vocab[char] for char in start_text.split()]], dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # 현재 시퀀스에 대한 예측\n",
        "            output = model(encoded)\n",
        "            # temperature sampling을 위한 logits 조정\n",
        "            logits = output[0, -1] / temperature\n",
        "            # 확률 분포 계산\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # 다음 토큰 샘플링\n",
        "            next_char_idx = torch.multinomial(probs, 1, replacement=True).item()\n",
        "            # TODO - Q15 : 생성된 텍스트에 새 문자 추가\n",
        "            current_text += \" \" + vocab_reverse[next_char_idx]\n",
        "            # 입력 시퀀스 업데이트\n",
        "            encoded = torch.cat([encoded[:, 1:], torch.tensor([[next_char_idx]])], dim=1)\n",
        "\n",
        "    return current_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJd790Fe3lvB",
        "outputId": "d9a399d0-f6bd-40a5-aea2-405a324521cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text :  1 5 8 3 3 3 3 422 422 47 629 629 629 629 629 629 629 932 932 932 932 932 932 932 932 932 932 932 615 615 615 615 548 548 548\n"
          ]
        }
      ],
      "source": [
        "# NOTE - 랜덤 출력 발생, 토큰 생성여부만 판단\n",
        "start_text = \"1 5 8 3 3\"\n",
        "generated_text = generate_text(model, start_text, vocab, vocab_reverse, max_length=30, temperature=1.0)\n",
        "print(\"Generated text : \", generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAk_uXTC3lvB"
      },
      "source": [
        "## Q 2.2. (10점)\n",
        "MultiHeadAttention 클래스에서 사용되는 Q(Query), K(Key), V(Value)의 역할을 각각 설명하세요.   \n",
        "특히, 각 행렬이 멀티 헤드 어텐션에서 입력 간의 관계를 계산하는 데 어떻게 기여하는지 서술해야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u88QOW1w3lvB"
      },
      "source": [
        "## A 2.2.\n",
        "1.Query (Q):\n",
        "Query는 현재 입력에 대한 질문 또는 관심의 초점을 나타냅니다.\n",
        "입력 시퀀스의 각 위치에서, 다른 위치들과의 유사도를 계산하기 위해 사용됩니다.\n",
        "MultiHeadAttention에서, Query 행렬은 입력 텐서를  W_q 라는 가중치 행렬로 변환하여 생성됩니다.\n",
        "\n",
        "2.Key (K):\n",
        "Key는 입력 시퀀스의 각 위치가 “어떤 정보”를 제공할 수 있는지 나타냅니다.\n",
        "Query와 Key 간의 내적(dot product)을 통해 입력 시퀀스 간의 유사성을 계산하며, 이를 통해 어느 위치가 현재 Query에 얼마나 중요한지 결정합니다.\n",
        "Key 행렬은 입력 텐서를  W_k 라는 가중치 행렬로 변환하여 생성됩니다.\n",
        "\n",
        "3.Value (V):\n",
        "Value는 실제로 전달하고자 하는 정보를 나타냅니다.\n",
        "Query와 Key를 통해 계산된 가중치(attention scores)를 Value에 적용하여 최종적으로 중요한정보를 강조하거나 덜 중요하게 만듭니다.\n",
        "Value 행렬은 입력 텐서를  W_v 라는 가중치 행렬로 변환하여 생성됩니다.\n",
        "\n",
        "  Q, K, V의 관계:\n",
        "  Query와 Key 간의 내적은 각 위치의 유사도를 측정하여 “어텐션 스코어”를 생성합니다.\n",
        "  어텐션 스코어를 소프트맥스를 통해 확률 값으로 변환한 후, Value에 곱하여 특정 위치의 정보가 얼마나 중요한지를 반영합니다.\n",
        "  이를 통해 모델은 입력 데이터의 특정 위치 간 관계를 학습합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy2vol0E3lvB"
      },
      "source": [
        "## Q 2.3. (10점)\n",
        "MultiHeadAttention에서 여러 개의 어텐션 헤드를 사용하는 이유와, 각 어텐션 헤드가 Q, K, V를 통해 서로 다른 관계를 학습할 수 있는 이유를 설명하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ruppwv403lvB"
      },
      "source": [
        "## A 2.3.\n",
        "\t1.여러 어텐션 헤드를 사용하는 이유:\n",
        "\tMultiHeadAttention은 하나의 어텐션 메커니즘으로 학습할 수 있는 관계가 제한적일 수 있다는 문제를 해결합니다.\n",
        "\t여러 어텐션 헤드를 사용하면 서로 다른 서브 공간(subspace)에서 Query, Key, Value를 독립적으로 학습할 수 있으므로, 다양한 관계와 패턴을 병렬로 학습할 수 있습니다.\n",
        "\t예를 들어, 한 어텐션 헤드는 입력 시퀀스의 문법적 관계를 학습하고, 다른 어텐션 헤드는 문맥적 관계를 학습할 수 있습니다.\n",
        "  \n",
        "\t2.각 어텐션 헤드가 Q, K, V를 통해 학습하는 관계:\n",
        "\t각 어텐션 헤드는 고유한 가중치 행렬  W_q, W_k, W_v 를 갖고 있어 입력 데이터를 서로 다른 방식으로 변환합니다.\n",
        "\t이를 통해 동일한 입력 데이터라도 각 헤드가 다른 관계를 학습할 수 있습니다.\n",
        "\t예를 들어, 하나의 헤드는 문장 내 단어 간의 짧은 거리 관계를 학습하고, 다른 헤드는 장거리 의존성을 학습할 수 있습니다.\n",
        "\t다양한 헤드가 독립적으로 학습한 정보를 병합(Concatenate)함으로써 더 풍부하고 다양한 표현을 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMoWsDe33lvB"
      },
      "source": [
        "## Bonus (1점)\n",
        "과제에 대한 피드백을 남겨주세요!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwARK8S13lvB"
      },
      "source": [
        "수고하셨습니다 :)  \n",
        "제출 전 셀을 모두 실행 후 제출 바랍니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJP4VO5d3lvC",
        "outputId": "a616abeb-1063-40f5-cf48-9c5f34124099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:END\n"
          ]
        }
      ],
      "source": [
        "# NOTE - 해당 셀 수정 금지\n",
        "logger.info(\"END\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "NLP_proj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}